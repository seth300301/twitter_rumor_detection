{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-12T11:17:16.134706Z","iopub.execute_input":"2022-05-12T11:17:16.135048Z","iopub.status.idle":"2022-05-12T11:17:16.309857Z","shell.execute_reply.started":"2022-05-12T11:17:16.134965Z","shell.execute_reply":"2022-05-12T11:17:16.309071Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import json\nimport csv\nimport re\nimport torch\nimport pandas as pd\nimport time\nimport operator\nimport math\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom transformers import BertModel, BertTokenizer, BertPreTrainedModel, AdamW\nfrom sklearn import preprocessing\nfrom sklearn.utils.class_weight import compute_class_weight","metadata":{"execution":{"iopub.status.busy":"2022-05-12T11:17:16.886866Z","iopub.execute_input":"2022-05-12T11:17:16.887115Z","iopub.status.idle":"2022-05-12T11:17:23.980280Z","shell.execute_reply.started":"2022-05-12T11:17:16.887088Z","shell.execute_reply":"2022-05-12T11:17:23.979567Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"Task 1","metadata":{}},{"cell_type":"code","source":"DEV_FILE = '/kaggle/input/preprocessed-data/dev.csv'\nTRAIN_FILE = '/kaggle/input/preprocessed-data/train.csv'\nTEST_FILE = '/kaggle/input/preprocessed-data/test.csv'\n\n# over-sampling non-rumour data\ntrain_set = pd.read_csv(TRAIN_FILE)\ntrain_set_rumour_idx = train_set.index[train_set['Label'] == 'rumour'].tolist()\nratio = math.floor(train_set['Label'].value_counts()[0]/train_set['Label'].value_counts()[1])\nwhile ratio:\n    train_set = pd.concat([train_set, train_set.loc[train_set_rumour_idx]], ignore_index=True, axis=0)\n    ratio -= 1\ntrain_set.to_csv('./train.csv', index = False)\nTRAIN_FILE = '/kaggle/working/train.csv'\n\ndev_set = pd.read_csv(DEV_FILE)\ndev_set_rumour_idx = dev_set.index[dev_set['Label'] == 'rumour'].tolist()\nratio = math.floor(dev_set['Label'].value_counts()[0]/dev_set['Label'].value_counts()[1])\nwhile ratio:\n    dev_set = pd.concat([dev_set, dev_set.loc[dev_set_rumour_idx]], ignore_index=True, axis=0)\n    ratio -= 1\ndev_set.to_csv('./dev.csv', index = False)\nDEV_FILE = '/kaggle/working/dev.csv'","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:03:46.222967Z","iopub.execute_input":"2022-05-12T08:03:46.22327Z","iopub.status.idle":"2022-05-12T08:03:46.521346Z","shell.execute_reply.started":"2022-05-12T08:03:46.223225Z","shell.execute_reply":"2022-05-12T08:03:46.520312Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# converts 'nonrumour' to 0 and 'rumour' to 1 in the list of labels in both training and development sets\nlabel_encoder = preprocessing.LabelEncoder()\nlabel_encoder.fit(['nonrumour', 'rumour'])\n\ntrain_events = pd.read_csv(TRAIN_FILE)\ntrain_events['Label'] = label_encoder.transform(train_events['Label'])\ndev_events = pd.read_csv(DEV_FILE)\ndev_events['Label'] = label_encoder.transform(dev_events['Label'])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:03:46.524086Z","iopub.execute_input":"2022-05-12T08:03:46.524363Z","iopub.status.idle":"2022-05-12T08:03:46.590555Z","shell.execute_reply.started":"2022-05-12T08:03:46.524323Z","shell.execute_reply":"2022-05-12T08:03:46.589623Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# compute the class weights for the loss function later on\n#class_weights = compute_class_weight(class_weight = 'balanced', classes = list(set(train_events['Label'])), y = train_events['Label'])\n#weights = torch.tensor(class_weights, dtype = torch.float)\n#weights = weights.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:03:46.592301Z","iopub.execute_input":"2022-05-12T08:03:46.592589Z","iopub.status.idle":"2022-05-12T08:03:46.598939Z","shell.execute_reply.started":"2022-05-12T08:03:46.592548Z","shell.execute_reply":"2022-05-12T08:03:46.597988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def text_split(text):\n    #text = text.replace('[CLS]','')\n    #text = text.replace('[SEP]','')\n    new_text = ''\n    curr_text = ''\n    \n    for word in text.split():\n        curr_text += word + ' '\n        \n        if word == '[SEP]':\n            new_text += curr_text\n            curr_text = ''\n            \n    new_text = new_text[:-1]\n    \n    return new_text\n\nsplit_train_events = train_events.copy()\nsplit_train_events['Event Tweets'] = split_train_events['Event Tweets'].apply(text_split)\nsplit_dev_events = dev_events.copy()\nsplit_dev_events['Event Tweets'] = split_dev_events['Event Tweets'].apply(text_split)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:03:46.602192Z","iopub.execute_input":"2022-05-12T08:03:46.602742Z","iopub.status.idle":"2022-05-12T08:03:46.859249Z","shell.execute_reply.started":"2022-05-12T08:03:46.602694Z","shell.execute_reply":"2022-05-12T08:03:46.858281Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# finds the max length of all the tweets for both the training and development\ntrain_max = 0\ncurr_count = 0\n\nfor i in split_train_events['Event Tweets']:\n    for j in i.split():\n        curr_count += 1\n        if j == '[SEP]':\n            if curr_count > train_max:\n                train_max = curr_count\n            curr_count = 0\n        \ndev_max = 0\ncurr_count = 0\n\nfor i in split_dev_events['Event Tweets']:\n    for j in i.split():\n        curr_count += 1\n        if j == '[SEP]':\n            if curr_count > dev_max:\n                dev_max = curr_count\n            curr_count = 0\n        \n# max counts used for padding later on\nprint(\"Training set has a max sentence length of\", train_max)\nprint(\"Development set has a max sentence length of\", dev_max)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:03:46.860749Z","iopub.execute_input":"2022-05-12T08:03:46.861048Z","iopub.status.idle":"2022-05-12T08:03:47.045082Z","shell.execute_reply.started":"2022-05-12T08:03:46.860994Z","shell.execute_reply":"2022-05-12T08:03:47.043771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# our main BERT classifier model\nclass BertClassifier(nn.Module):\n\n    def __init__(self, bert_base):\n        super(BertClassifier, self).__init__()\n        self.bert = bert_base\n        self.dropout = nn.Dropout(0.3)\n        self.relu = nn.ReLU()\n        self.in = nn.Linear(768,512)\n        self.out = nn.Linear(512, 2)\n        self.softmax = nn.LogSoftmax(dim = 1)\n\n    def forward(self, input_ids, attention_mask):\n        \n        _, inputs = self.bert(input_ids, attention_mask = attention_mask, return_dict = False)\n        \n        x = self.in(inputs)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.out(x)\n        x = self.softmax(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:03:47.047067Z","iopub.execute_input":"2022-05-12T08:03:47.047371Z","iopub.status.idle":"2022-05-12T08:03:47.056222Z","shell.execute_reply.started":"2022-05-12T08:03:47.04733Z","shell.execute_reply":"2022-05-12T08:03:47.055236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bert_base = BertModel.from_pretrained('bert-base-uncased')\nmodel = BertClassifier(bert_base)\nmodel.to('cuda')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n\n# batch_encode_plus tokenizes and prepares a list of ids using the BERT word embeddings dictionary and attention masks\n# which include padding with a max length to consider the padding for\ntrain_tokens = tokenizer.batch_encode_plus(\n    list(split_train_events['Event Tweets']),\n    add_special_tokens = False,\n    max_length = train_max,\n    pad_to_max_length = True,\n    truncation = True\n)\n\ndev_tokens = tokenizer.batch_encode_plus(\n    list(split_dev_events['Event Tweets']),\n    add_special_tokens = False,\n    max_length = dev_max,\n    pad_to_max_length = True,\n    truncation = True\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:04:10.23681Z","iopub.execute_input":"2022-05-12T08:04:10.23731Z","iopub.status.idle":"2022-05-12T08:04:48.400666Z","shell.execute_reply.started":"2022-05-12T08:04:10.237265Z","shell.execute_reply":"2022-05-12T08:04:48.399614Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert the lists of ids, attention masks, and labels to tensors to be read into tensor datasets to form\n# the samplers and data loaders\ntrain_ids = torch.tensor(train_tokens['input_ids'])\ntrain_mask = torch.tensor(train_tokens['attention_mask'])\ntrain_y = torch.tensor((list(split_train_events['Label'])))\ntrain_data = TensorDataset(train_ids, train_mask, train_y)\ntrain_sampler = RandomSampler(train_data)\ntrain_loader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n\ndev_ids = torch.tensor(dev_tokens['input_ids'])\ndev_mask = torch.tensor(dev_tokens['attention_mask'])\ndev_y = torch.tensor(list(split_dev_events['Label']))\ndev_data = TensorDataset(dev_ids, dev_mask, dev_y)\ndev_sampler = RandomSampler(dev_data)\ndev_loader = DataLoader(dev_data, sampler = dev_sampler, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:04:48.402243Z","iopub.execute_input":"2022-05-12T08:04:48.403615Z","iopub.status.idle":"2022-05-12T08:04:48.47852Z","shell.execute_reply.started":"2022-05-12T08:04:48.403567Z","shell.execute_reply":"2022-05-12T08:04:48.477345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train():\n    model.train()\n    total_loss = 0.0\n    total_acc = 0.0\n    total = 0\n    \n    # for each batch\n    for step, (ids, masks, labels) in enumerate(train_loader):            \n        ids, masks, labels = ids.to('cuda'), masks.to('cuda'), labels.to('cuda')\n        \n        model.zero_grad() # clears gradients\n\n        preds = model(ids, masks)\n\n        loss = criterion(preds, labels)\n        \n        # l2 regularisation\n        l2_norm = sum(param.pow(2.0).sum() for param in model.parameters())\n        loss = loss + l_lambda * l2_norm\n        \n        # l1 regularisation\n        #l1_norm = sum(param.abs().sum() for param in model.parameters())\n        #loss = loss + l_lambda * l1_norm\n        \n        loss.backward() # computes gradient during neural network backward pass\n        total_loss = total_loss + loss.item()\n\n        # used to prevent the \"exploding gradients\" problem\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        \n        # update parameters\n        opti.step()\n\n        _, predicts = preds.max(1)\n        total += labels.size(0)\n        total_acc += predicts.eq(labels).sum().item()\n        \n    avg_loss = total_loss / len(train_loader)\n    total_acc = total_acc / total\n    \n    return avg_loss, total_acc","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:04:48.480353Z","iopub.execute_input":"2022-05-12T08:04:48.48076Z","iopub.status.idle":"2022-05-12T08:04:48.492783Z","shell.execute_reply.started":"2022-05-12T08:04:48.480682Z","shell.execute_reply":"2022-05-12T08:04:48.491763Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluate():\n    model.eval()\n    total_loss = 0.0\n    total_acc = 0.0\n    total = 0\n\n    # for each batch\n    for step, (ids, masks, labels) in enumerate(dev_loader):\n        ids, masks, labels = ids.to('cuda'), masks.to('cuda'), labels.to('cuda')\n\n        with torch.no_grad():\n            preds = model(ids, masks)\n            \n            loss = criterion(preds, labels)\n            \n            # l2 regularisation\n            l2_norm = sum(param.pow(2.0).sum() for param in model.parameters())\n            loss = loss + l_lambda * l2_norm\n            \n            # l1 regularisation\n            #l1_norm = sum(param.abs().sum() for param in model.parameters())\n            #loss = loss + l_lambda * l1_norm\n            \n            total_loss = total_loss + loss.item()\n            \n            _, predicts = preds.max(1)\n            total += labels.size(0)\n            total_acc += predicts.eq(labels).sum().item()\n        \n    avg_loss = total_loss / len(train_loader)\n    total_acc = total_acc / total\n    \n    return avg_loss, total_acc","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:04:48.494379Z","iopub.execute_input":"2022-05-12T08:04:48.495253Z","iopub.status.idle":"2022-05-12T08:04:48.510627Z","shell.execute_reply.started":"2022-05-12T08:04:48.495212Z","shell.execute_reply":"2022-05-12T08:04:48.50935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# other parameters for the training and evaluation process\nopti = AdamW(model.parameters(), lr = 2e-5)    # AdamW optimiser\ncriterion = nn.NLLLoss()                       # negative log-likelihood loss function\nl_lambda = 0.0001                              # lambda for l1/l2 regularisation\nmax_epochs = 10\nbatch_size = 3","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_dev_loss = float('inf')\n\ntrain_loses = []\ndev_loses = []\n\n# actual training and evaluation occurs here for each epoch\nfor ep in range(max_epochs):\n    print(\"Epoch \" + str(ep + 1) + \":\")\n    \n    train_loss, train_acc = train()\n    dev_loss, dev_acc = evaluate()\n    \n    # judges value based on development loss rather than development accuracy\n    if dev_loss < best_dev_loss:\n        best_dev_loss = dev_loss\n        torch.save(model.state_dict(), 'best_weights.pt') # saves the weights of the model of the current epoch\n        \n    train_loses.append(train_loss)\n    dev_loses.append(dev_loss)\n    \n    print(\"Training Loss:   \", train_loss, \"Training Accuracy:   \", train_acc)\n    print(\"Development Loss:\", dev_loss, \"Development Accuracy:\", dev_acc, \"\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:04:48.512478Z","iopub.execute_input":"2022-05-12T08:04:48.512838Z","iopub.status.idle":"2022-05-12T08:25:48.328252Z","shell.execute_reply.started":"2022-05-12T08:04:48.51278Z","shell.execute_reply":"2022-05-12T08:25:48.324592Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# same processes as for the training and development sets but for the test set\ntest_events = pd.read_csv(TEST_FILE)\nsplit_test_events = test_events.copy()\nsplit_test_events['Event Tweets'] = split_test_events['Event Tweets'].apply(text_split)\n\ntest_max = 0\ncurr_count = 0\n\nfor i in split_test_events['Event Tweets']:\n    for j in i.split():\n        curr_count += 1\n        if j == '[SEP]':\n            if curr_count > test_max:\n                test_max = curr_count\n            curr_count = 0\n            \nprint(\"Test set has a max sentence length of\", test_max)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:25:48.330202Z","iopub.execute_input":"2022-05-12T08:25:48.330532Z","iopub.status.idle":"2022-05-12T08:25:48.426299Z","shell.execute_reply.started":"2022-05-12T08:25:48.330489Z","shell.execute_reply":"2022-05-12T08:25:48.425246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tokens = tokenizer.batch_encode_plus(\n    list(split_test_events['Event Tweets']),\n    add_special_tokens = False,\n    max_length = train_max,\n    pad_to_max_length = True,\n    truncation = True\n)\n\ntest_ids = torch.tensor(test_tokens['input_ids'])\ntest_mask = torch.tensor(test_tokens['attention_mask'])\n\ntest_data = TensorDataset(test_ids, test_mask)\ntest_sampler = RandomSampler(test_data)\ntest_loader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:25:48.427968Z","iopub.execute_input":"2022-05-12T08:25:48.428547Z","iopub.status.idle":"2022-05-12T08:25:52.155638Z","shell.execute_reply.started":"2022-05-12T08:25:48.428501Z","shell.execute_reply":"2022-05-12T08:25:52.154683Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"weight_file = f'best_weights.pt'\nmodel.load_state_dict(torch.load(weight_file)) # loads the best recorded weights during the best epoch\n\nwith torch.no_grad():\n    preds = model(test_ids.to('cuda'), test_mask.to('cuda'))\n    preds = preds.detach().cpu().numpy() # gets the prediction labels from the GPU by pushing it to the CPU\n    preds = np.argmax(preds, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:25:52.157356Z","iopub.execute_input":"2022-05-12T08:25:52.157673Z","iopub.status.idle":"2022-05-12T08:25:53.353971Z","shell.execute_reply.started":"2022-05-12T08:25:52.157629Z","shell.execute_reply":"2022-05-12T08:25:53.352963Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/final.csv', 'w', newline='') as o:\n    csv_write = csv.writer(o)\n    csv_write.writerow(['Id', 'Predicted'])\n\n    for i in range(len(preds)):\n        idx = i\n        is_rumour = preds[i]\n        csv_write.writerow([idx, is_rumour])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T08:25:53.355732Z","iopub.execute_input":"2022-05-12T08:25:53.356234Z","iopub.status.idle":"2022-05-12T08:25:53.36623Z","shell.execute_reply.started":"2022-05-12T08:25:53.356189Z","shell.execute_reply":"2022-05-12T08:25:53.365061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Task 2","metadata":{}},{"cell_type":"code","source":"# to process covid data, no need to run any of the cells in Section 'Task 1', just those in this section\n# some of these cells are repeats from above as we will use them in this section\n# if 'Task 1' cells have been run skip running cells labelled \"SKIP\"\n# SKIP\ndef text_split(text):\n    #text = text.replace('[CLS]','')\n    #text = text.replace('[SEP]','')\n    new_text = ''\n    curr_text = ''\n    \n    for word in text.split():\n        curr_text += word + ' '\n        \n        if word == '[SEP]':\n            new_text += curr_text\n            curr_text = ''\n            \n    new_text = new_text[:-1]\n    \n    return new_text","metadata":{"execution":{"iopub.status.busy":"2022-05-12T11:17:25.909614Z","iopub.execute_input":"2022-05-12T11:17:25.909876Z","iopub.status.idle":"2022-05-12T11:17:25.915070Z","shell.execute_reply.started":"2022-05-12T11:17:25.909848Z","shell.execute_reply":"2022-05-12T11:17:25.914367Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# SKIP\nclass BertClassifier(nn.Module):\n\n    def __init__(self, bert_base):\n        super(BertClassifier, self).__init__()\n        self.bert = bert_base\n        self.dropout = nn.Dropout(0.3)\n        self.relu = nn.ReLU()\n        self.in = nn.Linear(768,512)\n        self.out = nn.Linear(512, 2)\n        self.softmax = nn.LogSoftmax(dim = 1)\n\n    def forward(self, input_ids, attention_mask):\n        \n        _, inputs = self.bert(input_ids, attention_mask = attention_mask, return_dict = False)\n        \n        x = self.in(inputs)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.out(x)\n        x = self.softmax(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-12T11:17:26.342002Z","iopub.execute_input":"2022-05-12T11:17:26.342514Z","iopub.status.idle":"2022-05-12T11:17:26.350020Z","shell.execute_reply.started":"2022-05-12T11:17:26.342476Z","shell.execute_reply":"2022-05-12T11:17:26.349369Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# SKIP\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\nbert_base = BertModel.from_pretrained('bert-base-uncased')\nmodel = BertClassifier(bert_base)\nmodel.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2022-05-12T11:17:26.859722Z","iopub.execute_input":"2022-05-12T11:17:26.860335Z","iopub.status.idle":"2022-05-12T11:17:47.658847Z","shell.execute_reply.started":"2022-05-12T11:17:26.860282Z","shell.execute_reply":"2022-05-12T11:17:47.658166Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# had to breakup the COVID dataset in 2 to be able to process it within the memory limit of 16GB\n# hence covid_1.csv and covid_2.csv\n#COVID_FILE = '/kaggle/input/preprocessed-data/covid_1.csv'\nCOVID_FILE = '/kaggle/input/preprocessed-data/covid_2.csv'\n\ncovid_events = pd.read_csv(COVID_FILE)\nsplit_covid_events = covid_events.copy()\nsplit_covid_events['Event Tweets'] = split_covid_events['Event Tweets'].apply(text_split)\n\nbatch_size = 1 # have to set to 1 as setting to 3 causes memory issues due to size of COVID data\ncovid_max = 30 # similarly with max length of padding, 67 is the actual max length in the COVID dataset\n#curr_count = 0\n\n#for i in split_covid_events['Event Tweets']:\n#    for j in i.split():\n#        curr_count += 1\n#        if j == '[SEP]':\n#            if curr_count > covid_max:\n#                covid_max = curr_count\n#            curr_count = 0\n#            \n#print(\"The COVID set has a max sentence length of\", covid_max)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T11:17:47.660580Z","iopub.execute_input":"2022-05-12T11:17:47.661024Z","iopub.status.idle":"2022-05-12T11:17:48.848050Z","shell.execute_reply.started":"2022-05-12T11:17:47.660987Z","shell.execute_reply":"2022-05-12T11:17:48.847314Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"covid_tokens = tokenizer.batch_encode_plus(\n    list(split_covid_events['Event Tweets']),\n    add_special_tokens = False,\n    max_length = covid_max,\n    pad_to_max_length = True,\n    truncation = True\n)\n\ncovid_ids = torch.tensor(covid_tokens['input_ids'])\ncovid_mask = torch.tensor(covid_tokens['attention_mask'])\n\ncovid_data = TensorDataset(covid_ids, covid_mask)\ncovid_sampler = RandomSampler(covid_data)\ncovid_loader = DataLoader(covid_data, sampler = covid_sampler, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T11:17:48.849493Z","iopub.execute_input":"2022-05-12T11:17:48.849727Z","iopub.status.idle":"2022-05-12T11:19:27.453659Z","shell.execute_reply.started":"2022-05-12T11:17:48.849694Z","shell.execute_reply":"2022-05-12T11:19:27.452895Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# use f'best_weights.pt' if 'Task 1' was ran, if not used the initially saved best weights of our best model\n#weight_file = f'best_weights.pt'\nweight_file = '/kaggle/input/preprocessed-data/best_weights.pt'\nmodel.load_state_dict(torch.load(weight_file))","metadata":{"execution":{"iopub.status.busy":"2022-05-12T11:19:27.455358Z","iopub.execute_input":"2022-05-12T11:19:27.455598Z","iopub.status.idle":"2022-05-12T11:19:31.365914Z","shell.execute_reply.started":"2022-05-12T11:19:27.455565Z","shell.execute_reply":"2022-05-12T11:19:31.365185Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"with torch.no_grad():\n    preds = model(covid_ids.to('cuda'), covid_mask.to('cuda'))\n    preds = preds.detach().cpu().numpy()\n    preds = np.argmax(preds, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-12T11:19:31.366999Z","iopub.execute_input":"2022-05-12T11:19:31.367231Z","iopub.status.idle":"2022-05-12T11:19:39.179246Z","shell.execute_reply.started":"2022-05-12T11:19:31.367197Z","shell.execute_reply":"2022-05-12T11:19:39.178508Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#with open('/kaggle/working/covid_preds_1.csv', 'w', newline='') as o:\nwith open('/kaggle/working/covid_preds_2.csv', 'w', newline='') as o:\n    csv_write = csv.writer(o)\n    csv_write.writerow(['Id', 'Predicted'])\n\n    for i in range(len(preds)):\n        idx = i\n        is_rumour = preds[i]\n        csv_write.writerow([idx, is_rumour])","metadata":{"execution":{"iopub.status.busy":"2022-05-12T11:21:50.202193Z","iopub.execute_input":"2022-05-12T11:21:50.202604Z","iopub.status.idle":"2022-05-12T11:21:50.225868Z","shell.execute_reply.started":"2022-05-12T11:21:50.202571Z","shell.execute_reply":"2022-05-12T11:21:50.225133Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}