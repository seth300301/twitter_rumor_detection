{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-10T04:41:46.477365Z","iopub.execute_input":"2022-05-10T04:41:46.477620Z","iopub.status.idle":"2022-05-10T04:41:46.565879Z","shell.execute_reply.started":"2022-05-10T04:41:46.477592Z","shell.execute_reply":"2022-05-10T04:41:46.565189Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"import json\nimport csv\nimport re\nimport torch\nimport pandas as pd\nimport time\nimport operator\nimport math\nfrom torch.utils.data import TensorDataset, DataLoader, RandomSampler\nfrom torch.optim import AdamW\nimport torch.nn as nn\nfrom torch.nn import CrossEntropyLoss, MSELoss\nfrom transformers import BertModel, BertTokenizer, BertConfig, BertPreTrainedModel, AdamW\nfrom sklearn import preprocessing\nfrom transformers.data.processors.utils import InputExample, DataProcessor","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:41:46.567671Z","iopub.execute_input":"2022-05-10T04:41:46.568059Z","iopub.status.idle":"2022-05-10T04:41:46.575308Z","shell.execute_reply.started":"2022-05-10T04:41:46.568021Z","shell.execute_reply":"2022-05-10T04:41:46.574622Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"DEV_FILE = '/kaggle/input/preprocessed-data/dev.csv'\nTRAIN_FILE = '/kaggle/input/preprocessed-data/train.csv'\nTEST_FILE = '/kaggle/input/preprocessed-data/test.csv'\n\n# over-sampling non-rumour data\ntrain_set = pd.read_csv(TRAIN_FILE)\ntrain_set_rumour_idx = train_set.index[train_set['Label'] == 'rumour'].tolist()\nratio = math.floor(train_set['Label'].value_counts()[0]/train_set['Label'].value_counts()[1])\nwhile ratio:\n    train_set = pd.concat([train_set, train_set.loc[train_set_rumour_idx]], ignore_index=True, axis=0)\n    ratio -= 1\ntrain_set.to_csv('./train.csv', index = False)\nTRAIN_FILE = '/kaggle/working/train.csv'\n\ndev_set = pd.read_csv(DEV_FILE)\ndev_set_rumour_idx = dev_set.index[dev_set['Label'] == 'rumour'].tolist()\nratio = math.floor(dev_set['Label'].value_counts()[0]/dev_set['Label'].value_counts()[1])\nwhile ratio:\n    dev_set = pd.concat([dev_set, dev_set.loc[dev_set_rumour_idx]], ignore_index=True, axis=0)\n    ratio -= 1\ndev_set.to_csv('./dev.csv', index = False)\nDEV_FILE = '/kaggle/working/dev.csv'","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:41:46.576559Z","iopub.execute_input":"2022-05-10T04:41:46.577369Z","iopub.status.idle":"2022-05-10T04:41:46.824869Z","shell.execute_reply.started":"2022-05-10T04:41:46.577336Z","shell.execute_reply":"2022-05-10T04:41:46.824092Z"},"trusted":true},"execution_count":33,"outputs":[]},{"cell_type":"code","source":"label_encoder = preprocessing.LabelEncoder()\nlabel_encoder.fit(['nonrumour', 'rumour']) # converts 'nonrumour' to 0 and 'rumour' to 1\n\ntrain_events = pd.read_csv(TRAIN_FILE)\ntrain_events['Label'] = label_encoder.transform(train_events['Label'])\ndev_events = pd.read_csv(DEV_FILE)\ndev_events['Label'] = label_encoder.transform(dev_events['Label'])","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:41:46.827022Z","iopub.execute_input":"2022-05-10T04:41:46.827452Z","iopub.status.idle":"2022-05-10T04:41:46.885403Z","shell.execute_reply.started":"2022-05-10T04:41:46.827411Z","shell.execute_reply":"2022-05-10T04:41:46.884730Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"def text_split(text):\n    #text = text.replace('[CLS]','')\n    #text = text.replace('[SEP]','')\n    new_text = ''\n    curr_text = ''\n    \n    for word in text.split():\n        curr_text += word + ' '\n        \n        if word == '[SEP]':\n            new_text += curr_text\n            curr_text = ''\n            \n    new_text = new_text[:-1]\n    \n    return new_text\n\nsplit_train_events = train_events.copy()\nsplit_train_events['Event Tweets'] = split_train_events['Event Tweets'].apply(text_split)\nsplit_dev_events = dev_events.copy()\nsplit_dev_events['Event Tweets'] = split_dev_events['Event Tweets'].apply(text_split)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:41:46.886693Z","iopub.execute_input":"2022-05-10T04:41:46.886948Z","iopub.status.idle":"2022-05-10T04:41:47.152938Z","shell.execute_reply.started":"2022-05-10T04:41:46.886914Z","shell.execute_reply":"2022-05-10T04:41:47.152145Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"train_max = 0\ncurr_count = 0\n\nfor i in split_train_events['Event Tweets']:\n    for j in i.split():\n        curr_count += 1\n        if j == '[SEP]':\n            if curr_count > train_max:\n                train_max = curr_count\n            curr_count = 0\n        \ndev_max = 0\ncurr_count = 0\n\nfor i in split_dev_events['Event Tweets']:\n    for j in i.split():\n        curr_count += 1\n        if j == '[SEP]':\n            if curr_count > dev_max:\n                dev_max = curr_count\n            curr_count = 0\n        \n# max counts used for padding later on\nprint(\"Training set has a max sentence length of\", train_max)\nprint(\"Development set has a max sentence length of\", dev_max)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:41:47.154370Z","iopub.execute_input":"2022-05-10T04:41:47.154629Z","iopub.status.idle":"2022-05-10T04:41:47.322726Z","shell.execute_reply.started":"2022-05-10T04:41:47.154596Z","shell.execute_reply":"2022-05-10T04:41:47.321866Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"class BertClassifier(nn.Module):\n\n    def __init__(self, bert_base):\n        super(BertClassifier, self).__init__()\n        self.bert = bert_base\n        self.dropout = nn.Dropout(0.1)\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(768,512)\n        self.fc2 = nn.Linear(512, 2)\n        self.softmax = nn.LogSoftmax(dim = 1)\n\n    def forward(self, input_ids, attention_mask):\n        \n        _, cls_rep = self.bert(input_ids, attention_mask = attention_mask, return_dict = False)\n        \n        x = self.fc1(cls_rep)\n        x = self.relu(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.softmax(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:41:47.324280Z","iopub.execute_input":"2022-05-10T04:41:47.324602Z","iopub.status.idle":"2022-05-10T04:41:47.333832Z","shell.execute_reply.started":"2022-05-10T04:41:47.324566Z","shell.execute_reply":"2022-05-10T04:41:47.332949Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"bert_base = BertModel.from_pretrained('bert-base-uncased')\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n\ntrain_tokens = tokenizer.batch_encode_plus(\n    list(split_train_events['Event Tweets']),\n    add_special_tokens = False,\n    max_length = train_max,\n    pad_to_max_length = True,\n    truncation = True\n)\n\ndev_tokens = tokenizer.batch_encode_plus(\n    list(split_dev_events['Event Tweets']),\n    add_special_tokens = False,\n    max_length = dev_max,\n    pad_to_max_length = True,\n    truncation = True\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:41:47.335459Z","iopub.execute_input":"2022-05-10T04:41:47.335724Z","iopub.status.idle":"2022-05-10T04:42:21.512141Z","shell.execute_reply.started":"2022-05-10T04:41:47.335689Z","shell.execute_reply":"2022-05-10T04:42:21.511416Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"model = BertClassifier(bert_base)\nmodel.to('cuda')","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:42:21.513338Z","iopub.execute_input":"2022-05-10T04:42:21.514096Z","iopub.status.idle":"2022-05-10T04:42:21.646204Z","shell.execute_reply.started":"2022-05-10T04:42:21.514053Z","shell.execute_reply":"2022-05-10T04:42:21.645324Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"opti = AdamW(model.parameters(), lr = 2e-5)\ncriterion = nn.NLLLoss()\nmax_epochs = 10\nbatch_size = 3","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:42:21.649111Z","iopub.execute_input":"2022-05-10T04:42:21.649437Z","iopub.status.idle":"2022-05-10T04:42:21.660480Z","shell.execute_reply.started":"2022-05-10T04:42:21.649398Z","shell.execute_reply":"2022-05-10T04:42:21.659528Z"},"trusted":true},"execution_count":40,"outputs":[]},{"cell_type":"code","source":"train_ids = torch.tensor(train_tokens['input_ids'])\ntrain_mask = torch.tensor(train_tokens['attention_mask'])\ntrain_y = torch.tensor((list(split_train_events['Label'])))\ntrain_data = TensorDataset(train_ids, train_mask, train_y)\ntrain_sampler = RandomSampler(train_data)\ntrain_loader = DataLoader(train_data, sampler = train_sampler, batch_size = batch_size)\n\ndev_ids = torch.tensor(dev_tokens['input_ids'])\ndev_mask = torch.tensor(dev_tokens['attention_mask'])\ndev_y = torch.tensor(list(split_dev_events['Label']))\ndev_data = TensorDataset(dev_ids, dev_mask, dev_y)\ndev_sampler = RandomSampler(dev_data)\ndev_loader = DataLoader(dev_data, sampler = dev_sampler, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:42:21.662332Z","iopub.execute_input":"2022-05-10T04:42:21.662584Z","iopub.status.idle":"2022-05-10T04:42:21.734924Z","shell.execute_reply.started":"2022-05-10T04:42:21.662551Z","shell.execute_reply":"2022-05-10T04:42:21.734305Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"def train():\n    \n    model.train()\n    total_loss = 0.0\n    total_acc = 0.0\n    all_preds = []\n    \n    for step, (ids, masks, labels) in enumerate(train_loader):            \n        ids, masks, labels = ids.to('cuda'), masks.to('cuda'), labels.to('cuda')\n        \n        model.zero_grad()\n\n        preds = model(ids, masks)\n\n        loss = criterion(preds, labels)\n        total_loss = total_loss + loss.item()\n        loss.backward()\n\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        opti.step()\n\n        preds = preds.detach().cpu().numpy()\n        all_preds.append(preds)\n\n    avg_loss = total_loss / len(train_loader)\n    all_preds = np.concatenate(all_preds, axis = 0)\n    \n    return avg_loss, all_preds","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:42:21.735907Z","iopub.execute_input":"2022-05-10T04:42:21.736111Z","iopub.status.idle":"2022-05-10T04:42:21.744757Z","shell.execute_reply.started":"2022-05-10T04:42:21.736085Z","shell.execute_reply":"2022-05-10T04:42:21.744110Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"def evaluate():\n    model.eval()\n    total_loss = 0.0\n    total_acc = 0.0\n    all_preds = []\n    \n    for step, (ids, masks, labels) in enumerate(dev_loader):\n        ids, masks, labels = ids.to('cuda'), masks.to('cuda'), labels.to('cuda')\n\n        with torch.no_grad():\n            preds = model(ids, masks)\n            \n            loss = criterion(preds, labels)\n            total_loss = total_loss + loss.item()\n            \n            preds = preds.detach().cpu().numpy()\n            all_preds.append(preds)\n    \n    avg_loss = total_loss / len(dev_loader)\n    all_preds = np.concatenate(all_preds, axis = 0)\n    \n    return avg_loss, all_preds","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:42:21.745837Z","iopub.execute_input":"2022-05-10T04:42:21.746609Z","iopub.status.idle":"2022-05-10T04:42:21.756400Z","shell.execute_reply.started":"2022-05-10T04:42:21.746567Z","shell.execute_reply":"2022-05-10T04:42:21.755648Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"best_dev_loss = float('inf')\n\ntrain_loses = []\ndev_loses = []\n\nfor ep in range(max_epochs):\n    print(\"Epoch \" + str(ep + 1) + \":\")\n    \n    train_loss, _ = train()\n    dev_loss, _ = evaluate()\n    \n    if dev_loss < best_dev_loss:\n        best_dev_loss = dev_loss\n        torch.save(model.state_dict(), 'best_weights.pt')\n        \n    train_loses.append(train_loss)\n    dev_loses.append(dev_loss)\n    \n    print(f\"\\nTraining Loss: {train_loss:.3f}\")\n    print(f\"\\nDevelopment Loss: {dev_loss:.3f}\")","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:42:21.757566Z","iopub.execute_input":"2022-05-10T04:42:21.758280Z","iopub.status.idle":"2022-05-10T04:52:06.469562Z","shell.execute_reply.started":"2022-05-10T04:42:21.758226Z","shell.execute_reply":"2022-05-10T04:52:06.468772Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"test_events = pd.read_csv(TEST_FILE)\nsplit_test_events = test_events.copy()\nsplit_test_events['Event Tweets'] = split_test_events['Event Tweets'].apply(text_split)\n\ntest_max = 0\ncurr_count = 0\n\nfor i in split_test_events['Event Tweets']:\n    for j in i.split():\n        curr_count += 1\n        if j == '[SEP]':\n            if curr_count > test_max:\n                test_max = curr_count\n            curr_count = 0\n            \nprint(\"Test set has a max sentence length of\", test_max)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:52:43.449731Z","iopub.execute_input":"2022-05-10T04:52:43.450176Z","iopub.status.idle":"2022-05-10T04:52:43.515660Z","shell.execute_reply.started":"2022-05-10T04:52:43.450136Z","shell.execute_reply":"2022-05-10T04:52:43.514936Z"},"trusted":true},"execution_count":49,"outputs":[]},{"cell_type":"code","source":"test_tokens = tokenizer.batch_encode_plus(\n    list(split_test_events['Event Tweets']),\n    add_special_tokens = False,\n    max_length = train_max,\n    pad_to_max_length = True,\n    truncation = True\n)\n\ntest_ids = torch.tensor(test_tokens['input_ids'])\ntest_mask = torch.tensor(test_tokens['attention_mask'])\n\ntest_data = TensorDataset(test_ids, test_mask)\ntest_sampler = RandomSampler(test_data)\ntest_loader = DataLoader(test_data, sampler = test_sampler, batch_size = batch_size)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:52:45.038130Z","iopub.execute_input":"2022-05-10T04:52:45.038392Z","iopub.status.idle":"2022-05-10T04:52:48.488731Z","shell.execute_reply.started":"2022-05-10T04:52:45.038364Z","shell.execute_reply":"2022-05-10T04:52:48.488026Z"},"trusted":true},"execution_count":50,"outputs":[]},{"cell_type":"code","source":"weight_file = f'best_weights.pt'\nmodel.load_state_dict(torch.load(weight_file))\n\nwith torch.no_grad():\n    preds = model(test_ids.to('cuda'), test_mask.to('cuda'))\n    preds = preds.detach().cpu().numpy()\n    preds = np.argmax(preds, axis = 1)","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:55:36.114890Z","iopub.execute_input":"2022-05-10T04:55:36.115569Z","iopub.status.idle":"2022-05-10T04:55:37.269186Z","shell.execute_reply.started":"2022-05-10T04:55:36.115531Z","shell.execute_reply":"2022-05-10T04:55:37.268460Z"},"trusted":true},"execution_count":60,"outputs":[]},{"cell_type":"code","source":"with open('/kaggle/working/final.csv', 'w', newline='') as o:\n    csv_write = csv.writer(o)\n    csv_write.writerow(['Id', 'Predicted'])\n\n    for i in range(len(preds)):\n        idx = i\n        is_rumour = preds[i]\n        csv_write.writerow([idx, is_rumour])","metadata":{"execution":{"iopub.status.busy":"2022-05-10T04:55:43.420178Z","iopub.execute_input":"2022-05-10T04:55:43.420764Z","iopub.status.idle":"2022-05-10T04:55:43.428686Z","shell.execute_reply.started":"2022-05-10T04:55:43.420723Z","shell.execute_reply":"2022-05-10T04:55:43.427810Z"},"trusted":true},"execution_count":61,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}