{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import requests\n",
    "from requests.adapters import HTTPAdapter\n",
    "from requests.packages.urllib3.util.retry import Retry\n",
    "import errno\n",
    "import os\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lookup_dict(type='train', target='tweet'):\n",
    "    path = f'./{type}_{target}'\n",
    "    tweet2author = defaultdict(list)\n",
    "    author2tweet = defaultdict(str)\n",
    "    for file in os.listdir(path):\n",
    "        with open(os.path.join(path, file)) as f:\n",
    "            data = json.load(f)\n",
    "            if 'id' in data and target=='tweet':\n",
    "                tweet_id = data['id']\n",
    "            elif 'pinned_tweet_id' in data and target == 'user':\n",
    "                tweet_id = data['pinned_tweet_id']\n",
    "            else: \n",
    "                tweet_id = ''\n",
    "            if 'author_id' in data and target=='tweet':\n",
    "                author_id = data['author_id']\n",
    "            elif 'id' in data and target == 'user':\n",
    "                author_id = data['id']\n",
    "            else:\n",
    "                author_id = ''\n",
    "            \n",
    "            author2tweet.setdefault(author_id, []).append(tweet_id)\n",
    "            tweet2author.setdefault(tweet_id, author_id)\n",
    "\n",
    "                \n",
    "    with open(f'./tweet2author_{type}_{target}.json', 'w') as f:\n",
    "        json.dump(tweet2author, f)\n",
    "    with open(f'./author2tweet_{type}_{target}.json', 'w') as f:\n",
    "        json.dump(author2tweet, f)\n",
    "    return tweet2author, author2tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id_chunks(**kwargs):\n",
    "    ids = []\n",
    "    size = kwargs.get('size', 100)\n",
    "    source = kwargs.get('source', None)\n",
    "\n",
    "    if type(source) == defaultdict or type(source) == dict:\n",
    "        for k, v in source.items():\n",
    "            if type(v) == list and '' not in v:\n",
    "                ids.extend(v)\n",
    "            elif type(v) == str and v != '':\n",
    "                ids.append(v)\n",
    "                \n",
    "    elif type(source) == str:    \n",
    "        with open(source) as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                line = line.replace('\\n', '')\n",
    "                ids.extend(line.split(','))\n",
    "        \n",
    "    chunks = [ids[i:i+size] for i in range(0, len(ids), size)]\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_requests(url, params, headers, folder='train_tweet', proxies={}):\n",
    "    session = requests.Session()\n",
    "    retry = Retry(connect=3, backoff_factor=0.5)\n",
    "    adapter = HTTPAdapter(max_retries=retry)\n",
    "    session.mount('http://', adapter)\n",
    "    session.mount('https://', adapter)\n",
    "    \n",
    "    response = session.get(\n",
    "        url,\n",
    "        headers=headers, \n",
    "        params=params,\n",
    "        proxies=proxies)\n",
    "        \n",
    "    if 'status' in response.json() and response.json()['status']==429:\n",
    "        current_time = time.strftime('%H:%M:%S',time.localtime(time.time()))\n",
    "        print(f'{current_time}: rate limit exceeded, the program will sleep for 15 minutes')\n",
    "        time.sleep(60*15)\n",
    "        response = session.get(\n",
    "            url,\n",
    "            headers=headers, \n",
    "            params=params,\n",
    "            proxies=proxies)\n",
    "    \n",
    "    data = response.json()\n",
    "    \n",
    "    return handle_response(data, folder)  \n",
    "\n",
    "def handle_response(response, folder='train_tweet'):\n",
    "    if 'data' in response:\n",
    "        data = response['data']\n",
    "        for item in data:\n",
    "            id = item['id']\n",
    "            with open(f'./{folder}/{id}.json', 'w+') as f:\n",
    "                json.dump(item, f)\n",
    "        return len(data)\n",
    "    else:\n",
    "        with open(f'./log_{folder}.txt', 'a+') as f:\n",
    "            f.write(json.dumps(response, indent=4))\n",
    "        return 0  \n",
    "    \n",
    "    \n",
    "def data_harvester(chunks, folder, params, headers, target='tweets'):\n",
    "    total = 0\n",
    "    retrived = 0\n",
    "    if not os.path.exists(f'./{folder}'):\n",
    "        try:\n",
    "            os.makedirs(f'./{folder}')\n",
    "        except OSError as e: \n",
    "            if e.errno != errno.EEXIST:\n",
    "                raise\n",
    "            \n",
    "    for c in chunks:\n",
    "        total += len(c)\n",
    "        params['ids'] = ','.join(c).replace('\\n','')\n",
    "        try:\n",
    "            retrived += make_requests(\n",
    "                f'https://api.twitter.com/2/{target}/',\n",
    "                headers=headers, \n",
    "                params=params,\n",
    "                folder=folder)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        if total % 5000 == 0:\n",
    "            print(f'Seen {total} {target}, retrived {retrived}, retrive rate {round(retrived/total*100,2)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'ids': [],\n",
    "    'expansions': 'author_id,entities.mentions.username,geo.place_id,in_reply_to_user_id,referenced_tweets.id,referenced_tweets.id.author_id',\n",
    "    'place.fields': 'contained_within,country,country_code,full_name,geo,id,name,place_type',\n",
    "    'poll.fields': 'duration_minutes,end_datetime,id,options,voting_status',\n",
    "    'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,referenced_tweets,reply_settings,source,text,withheld',\n",
    "    'user.fields':'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld'}\n",
    "params_usr = {\n",
    "    'ids': [],\n",
    "    'expansions': 'pinned_tweet_id',\n",
    "    'tweet.fields': 'attachments,author_id,context_annotations,conversation_id,created_at,entities,geo,id,in_reply_to_user_id,lang,possibly_sensitive,public_metrics,referenced_tweets,reply_settings,source,text,withheld',\n",
    "    'user.fields':'created_at,description,entities,id,location,name,pinned_tweet_id,profile_image_url,protected,public_metrics,url,username,verified,withheld'}\n",
    "\n",
    "headers = {\n",
    "    'Authorization': 'Bearer AAAAAAAAAAAAAAAAAAAAAI%2FnbgEAAAAASNSbePMKDZ2%2BrcK%2BLf6h44gPjMY%3Dw0xoJvCQZUyHHTTV1qJnMhpKhfqNX5WGxGSoFCIBLxOH6Avi9U',\n",
    "}\n",
    "win_proxies = {'http': '127.0.0.1:1080',\n",
    " 'https': '127.0.0.1:1080',\n",
    " 'ftp': 'ftp://127.0.0.1:1080'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users pinned tweets in train set\n",
    "pinned_tweet2author_train, pinned_author2tweet_train = get_lookup_dict(target='user')\n",
    "# split ids into chunks\n",
    "chunks_tweet = get_id_chunks(source=pinned_author2tweet_train)\n",
    "# get and save json files \n",
    "data_harvester(chunks_tweet, params, headers, folder='train_user_pinned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users pinned tweets in dev set\n",
    "pinned_tweet2author_dev, pinned_author2tweet_dev = get_lookup_dict(type='dev', target='user')\n",
    "# split ids into chunks\n",
    "chunks_tweet = get_id_chunks(source=pinned_author2tweet_dev)\n",
    "# get and save json files \n",
    "data_harvester(chunks_tweet, params, headers, folder='dev_user_pinned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get covid tweet's user data\n",
    "with open('./author2tweet_covid.json') as f:\n",
    "    author2tweet = json.load(f)\n",
    "with open('./tweet2author_covid.json') as f:\n",
    "    tweet2author = json.load(f)\n",
    "    \n",
    "chunks_user = get_id_chunks(source=tweet2author)\n",
    "data_harvester(chunks_user, 'covid_user', params_usr, headers, target='users')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# users pinned tweets in covid set\n",
    "pinned_tweet2author_covid, pinned_author2tweet_covid = get_lookup_dict(type='covid', target='user')\n",
    "# split ids into chunks\n",
    "chunks_user_covid = get_id_chunks(source=pinned_author2tweet_covid)\n",
    "# get and save json files \n",
    "data_harvester(chunks_user_covid, 'covid_user_pinned', params, headers)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "43416e547f35e717d897ca7ea8e1a0079eaa697f8e9e8c8ed832eed461898d64"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
