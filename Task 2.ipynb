{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-05-12T15:13:09.011164Z","iopub.execute_input":"2022-05-12T15:13:09.011457Z","iopub.status.idle":"2022-05-12T15:13:09.046898Z","shell.execute_reply.started":"2022-05-12T15:13:09.011374Z","shell.execute_reply":"2022-05-12T15:13:09.045843Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"COVID = '/kaggle/input/task2analysis/covid.csv'\nLABEL1 = '/kaggle/input/task2analysis/covid_preds_1.csv'\nLABEL2 = '/kaggle/input/task2analysis/covid_preds_2.csv'","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\ncovid = pd.read_csv(COVID)\nlabel1 = pd.read_csv(LABEL1)\nlabel2 = pd.read_csv(LABEL2)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dic = {}\nlabel = list(label1['Predicted'])+list(label2['Predicted'])\nfor i in range(len(covid)):\n    dic[list(covid['Source_Tweet'])[i]] = label[i]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('There are', len(label), 'events')\nr_num = 0\nfor i in label:\n    if i == 1:\n        r_num += 1\nprint('There are', r_num, 'rumours and there are', len(label)-r_num, 'non-rumours')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rumour_id_list = []\nnon_rumour_id_list = []\ntweets = list(covid['Event Tweets'])\nrumour_tweet_list = []\nnon_rumour_tweet_list = []\n\nnum = 0\nfor i in dic.keys():\n    if dic[i] == 1:\n        rumour_id_list.append(i)\n        rumour_tweet_list.append(tweets[num])\n    else:\n        non_rumour_id_list.append(i)\n        non_rumour_tweet_list.append(tweets[num])\n    num += 1","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef reply(data, title):\n    rep_num = []\n    rep_max = 0\n    for i in range(len(data)):\n        rep_num.append(len(data[i].split('[SEP]'))-1)\n        if rep_max < len(data[i].split('[SEP]'))-1:\n            rep_max = len(data[i].split('[SEP]'))-1\n    p = np.arange(0, rep_max, 1)\n    plt.hist(rep_num, bins = p)\n    plt.xlabel(\"Number of Replied Tweet\")\n    plt.ylabel(\"Number of Events\")\n    plt.title(title)\n    plt.show()\n\nreply(rumour_tweet_list, 'Rumour')\nreply(non_rumour_tweet_list, 'Non-Rumour')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"events = []\nwith open(\"/kaggle/input/task2analysis/covid.data.txt\", \"r\") as f:\n    for line in f.readlines():\n        events.append(line[:-1].split(\",\"))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#import json\n\n#rumour_hashtags = {}\n#non_rumour_hashtags = {}\n\n#for i in range(len(events)):\n#    for j in range(len(events[i])):\n#        try:\n#            open('./covid/covid_tweet/'+events[i][j]+'.json')\n#            file = json.load(open('./covid/covid_tweet/' + events[i][j] + '.json'))\n#            for k in file.get('entities').get('hashtags'):\n#                l = k['tag']\n#                if int(events[i][0]) in rumour_id_list:\n#                    if l in rumour_hashtags.keys():\n#                        rumour_hashtags[l] += 1\n#                    else:\n#                        rumour_hashtags[l] = 1\n#                else:\n#                    if l in non_rumour_hashtags.keys():\n#                        non_rumour_hashtags[l] += 1\n#                    else:\n#                        non_rumour_hashtags[l] = 1\n#        except:\n#            continue\n\n#pd.DataFrame(rumour_hashtags,index=[1]).T.to_csv('rumour_hashtags.csv')\n#pd.DataFrame(non_rumour_hashtags,index=[1]).T.to_csv('non_rumour_hashtags.csv')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rumour_hashtags = {}\nnon_rumour_hashtags = {}\n\nR_HASHTAG = '/kaggle/input/task2analysis/rumour_hashtags.csv'\nN_HASHTAG = '/kaggle/input/task2analysis/non_rumour_hashtags.csv'\n\nr_hashtag = pd.read_csv(R_HASHTAG)\nn_hashtag = pd.read_csv(N_HASHTAG)\nfor i in range(len(r_hashtag)):\n    rumour_hashtags[list(r_hashtag['Unnamed: 0'])[i]] = list(r_hashtag['1'])[i]\n    non_rumour_hashtags[list(n_hashtag['Unnamed: 0'])[i]] = list(n_hashtag['1'])[i]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rumour_hashtags_p = {}\nnon_rumour_hashtags_p = {}\nfor i in rumour_hashtags.keys():\n    tag = i.lower()\n    if tag in rumour_hashtags_p:\n        rumour_hashtags_p[tag] += rumour_hashtags[i]\n    else:\n        rumour_hashtags_p[tag] = rumour_hashtags[i]\nfor i in non_rumour_hashtags.keys():\n    tag = i.lower()\n    if tag in non_rumour_hashtags_p:\n        non_rumour_hashtags_p[tag] += non_rumour_hashtags[i]\n    else:\n        non_rumour_hashtags_p[tag] = non_rumour_hashtags[i]\n\nrumour_hashtags_p = sorted(rumour_hashtags_p.items(), key = lambda k:k[1], reverse = True)\nnon_rumour_hashtags_p = sorted(non_rumour_hashtags_p.items(), key = lambda k:k[1], reverse = True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rumour_x = []\nrumour_y = []\nfor i in range(22):\n    rumour_x.insert(0, rumour_hashtags_p[i][0])\n    rumour_y.insert(0, rumour_hashtags_p[i][1])\n\nplt.barh(rumour_x, rumour_y)\nplt.rcParams[\"figure.dpi\"]=1000\nplt.xlabel(\"Number of Hashtags\")\nplt.title(\"Top 22 Hashtags of Rumours\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"non_rumour_x = []\nnon_rumour_y = []\nfor i in range(22):\n    non_rumour_x.insert(0, non_rumour_hashtags_p[i][0])\n    non_rumour_y.insert(0, non_rumour_hashtags_p[i][1])\n\nplt.barh(non_rumour_x, non_rumour_y)\nplt.rcParams[\"figure.dpi\"]=1000\nplt.xlabel(\"Number of Hashtags\")\nplt.title(\"Top 22 Hashtags of Non-Rumours\")\nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.close('all')\nplt.barh(non_rumour_x, non_rumour_y, color='#ED1C24', label='non-rumour hashtags')\nplt.barh(rumour_x, rumour_y, color='#0072BC', label='rumour hashtags')\nplt.legend(('rumour','non_rumour'), loc='lower right')\nplt.title('Rumours Hashtags VS Non-Rumours Hashtags')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"user_created_rumour = []\nuser_created_non_rumour = []\nfollowers_rumour = []\nfollowers_non_rumour = []\nfollowing_rumour = []\nfollowing_non_rumour = []\ntweet_num_rumour = []\ntweet_num_non_rumour = []\nverified_rumour = []\nverified_non_rumour = []\n\nfor i in rumour_id_list:\n    try:\n        open('./covid/covid_tweet/'+str(i)+'.json')\n        file = json.load(open('./covid/covid_tweet/' + str(i) + '.json'))\n        user_id = file.get('author_id')\n        file1 = json.load(open('./covid/covid_user/'+ user_id + '.json'))\n\n        user_created_rumour.append(int(file1.get('created_at')[:4]))\n        followers_rumour.append(file1.get('public_metrics').get('followers_count'))\n        following_rumour.append(file1.get('public_metrics').get('following_count'))\n        tweet_num_rumour.append(file1.get('public_metrics').get('tweet_count'))\n        if file1.get('verified') == True:\n            verified_rumour.append(1)\n        else:\n            verified_rumour.append(0)\n    except:\n        continue\n        \nfor i in non_rumour_id_list:\n    try:\n        open('./covid/covid_tweet/'+str(i)+'.json')\n        file = json.load(open('./covid/covid_tweet/' + str(i) + '.json'))\n        user_id = file.get('author_id')\n        file1 = json.load(open('./covid/covid_user/'+ user_id + '.json'))\n\n        user_created_non_rumour.append(int(file1.get('created_at')[:4]))\n        followers_non_rumour.append(file1.get('public_metrics').get('followers_count'))\n        following_non_rumour.append(file1.get('public_metrics').get('following_count'))\n        tweet_num_non_rumour.append(file1.get('public_metrics').get('tweet_count'))\n        if file1.get('verified') == True:\n            verified_non_rumour.append(1)\n        else:\n            verified_non_rumour.append(0)\n    except:\n        continue","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('For Rumours:')\nprint('mean user created:', np.mean(user_created_rumour))\nprint('mean number of followers:', np.mean(followers_rumour))\nprint('mean number of following:', np.mean(following_rumour))\nprint('mean number of tweet:', np.mean(tweet_num_rumour))\nprint('user vertified or not:', np.mean(verified_rumour))\n\nprint('For Non-Rumours:')\nprint('mean user created:', np.mean(user_created_non_rumour))\nprint('mean number of followers:', np.mean(followers_non_rumour))\nprint('mean number of following:', np.mean(following_non_rumour))\nprint('mean number of tweet:', np.mean(tweet_num_non_rumour))\nprint('user vertified or not:', np.mean(verified_non_rumour))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\nsen_tweet_rumour = []\nsen_tweet_non_rumour = []\nfor i in rumour_tweet_list:\n    sen_tweet_rumour.append(i.split('[SEP]')[0][5:].strip())\nfor i in non_rumour_tweet_list:\n    sen_tweet_non_rumour.append(i.split('[SEP]')[0][5:].strip())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"analyzer = SentimentIntensityAnalyzer()\n\nneg_r = []\nneu_r = []\npos_r = []\nfor sentence in sen_tweet_rumour:\n    neg_r.append(analyzer.polarity_scores(sentence)['neg'])\n    neu_r.append(analyzer.polarity_scores(sentence)['neu'])\n    pos_r.append(analyzer.polarity_scores(sentence)['pos'])\n\nneg_n = []\nneu_n = []\npos_n = []\ncom_n = []\nfor sentence in sen_tweet_non_rumour:\n    neg_n.append(analyzer.polarity_scores(sentence)['neg'])\n    neu_n.append(analyzer.polarity_scores(sentence)['neu'])\n    pos_n.append(analyzer.polarity_scores(sentence)['pos'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('For Rumour:')\nprint('Mean score of Negative:', np.mean(neg_r))\nprint('Mean score of Neutral:', np.mean(neu_r))\nprint('Mean score of Positive:', np.mean(pos_r))\n\nprint('For Non-Rumour:')\nprint('Mean score of Negative:', np.mean(neg_n))\nprint('Mean score of Neutral:', np.mean(neu_n))\nprint('Mean score of Positive:', np.mean(pos_n))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sen_reply_rumour = []\nsen_reply_non_rumour = []\nfor i in rumour_tweet_list:\n    if i.split('[SEP]')[1:-1] != []:\n        for j in i.split('[SEP]')[1:-1]:\n            sen_reply_rumour.append(j.strip())\nfor i in non_rumour_tweet_list:\n    if i.split('[SEP]')[1:-1] != []:\n        for j in i.split('[SEP]')[1:-1]:\n            sen_reply_non_rumour.append(j.strip())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"neg_r_reply = []\nneu_r_reply = []\npos_r_reply = []\nfor sentence in sen_reply_rumour:\n    neg_r_reply.append(analyzer.polarity_scores(sentence)['neg'])\n    neu_r_reply.append(analyzer.polarity_scores(sentence)['neu'])\n    pos_r_reply.append(analyzer.polarity_scores(sentence)['pos'])\n\nneg_n_reply = []\nneu_n_reply = []\npos_n_reply = []\ncom_n_reply = []\nfor sentence in sen_reply_non_rumour:\n    neg_n_reply.append(analyzer.polarity_scores(sentence)['neg'])\n    neu_n_reply.append(analyzer.polarity_scores(sentence)['neu'])\n    pos_n_reply.append(analyzer.polarity_scores(sentence)['pos'])","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('For Rumour Replies:')\nprint('Mean score of Negative:', np.mean(neg_r_reply))\nprint('Mean score of Neutral:', np.mean(neu_r_reply))\nprint('Mean score of Positive:', np.mean(pos_r_reply))\n\nprint('For Non-Rumour Replies:')\nprint('Mean score of Negative:', np.mean(neg_n_reply))\nprint('Mean score of Neutral:', np.mean(neu_n_reply))\nprint('Mean score of Positive:', np.mean(pos_n_reply))","metadata":{},"execution_count":null,"outputs":[]}]}