{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "7f5e83ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import pandas as pd\n",
    "from transformers import BertModel, BertTokenizerFast, Trainer, TrainingArguments\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "\n",
    "train_data_path = './train_dev_data/train/'\n",
    "dev_data_path = './train_dev_data/dev/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "85a2394d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1895 events in the training set ( 420 rumour , 1475  non_rumour )\n",
      "There are 632 events in the development set ( 139 rumour , 493  non_rumour )\n"
     ]
    }
   ],
   "source": [
    "train_events = 0\n",
    "train_rumour = 0\n",
    "train_non_rumour = 0\n",
    "labels_train = []\n",
    "\n",
    "dev_events = 0\n",
    "dev_rumour = 0\n",
    "dev_non_rumour = 0\n",
    "labels_dev = []\n",
    "\n",
    "with open(\"./project-data/train.label.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        labels_train.append(line[:-1])\n",
    "        train_events += 1\n",
    "        if line[:-1] == 'rumour':\n",
    "            train_rumour += 1\n",
    "        else:\n",
    "            train_non_rumour += 1\n",
    "            \n",
    "with open(\"./project-data/dev.label.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        labels_dev.append(line[:-1])\n",
    "        dev_events += 1\n",
    "        if line[:-1] == 'rumour':\n",
    "            dev_rumour += 1\n",
    "        else:\n",
    "            dev_non_rumour += 1\n",
    "    \n",
    "print(\"There are\", train_events, \"events in the training set (\", train_rumour, \"rumour\", \",\", train_non_rumour, \" non_rumour )\")\n",
    "print(\"There are\", dev_events, \"events in the development set (\", dev_rumour, \"rumour\", \",\", dev_non_rumour, \" non_rumour )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "d2c93dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "events_train = []\n",
    "events_dev = []\n",
    "\n",
    "with open(\"./project-data/train.data.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        events_train.append(line[:-1].split(\",\"))\n",
    "        \n",
    "with open(\"./project-data/dev.data.txt\", \"r\") as f:\n",
    "    for line in f.readlines():\n",
    "        events_dev.append(line[:-1].split(\",\"))\n",
    "\n",
    "#events = lists of all events, the first id is the source id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d6d12c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweet_events = []\n",
    "index = 0\n",
    "\n",
    "for i in range(len(events_train)):\n",
    "    event = dict()\n",
    "    event['index'] = index\n",
    "    event['ids'] = events_train[i]\n",
    "    event['is_rumour'] = labels_train[i]\n",
    "    index += 1\n",
    "    train_tweet_events.append(event)\n",
    "\n",
    "dev_tweet_events = []\n",
    "index = 0\n",
    "\n",
    "for i in range(len(events_dev)):\n",
    "    event = dict()\n",
    "    event['index'] = index\n",
    "    event['ids'] = events_dev[i]\n",
    "    event['is_rumour'] = labels_dev[i]\n",
    "    index += 1\n",
    "    dev_tweet_events.append(event)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4677b204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_extraction(tweet):\n",
    "    # compiling features of each tweet object\n",
    "    text = tweet['text']\n",
    "    text = re.sub('@[\\S]+', '', text).lower() #remove @mention\n",
    "    text = re.sub('https://[\\S]+', '', text) #remove url\n",
    "    text = re.sub('http://[\\S]+', '', text)\n",
    "    text = re.sub('[\\n]+', ' ', text).strip() #remove \\n\n",
    "\n",
    "    '''lang = tweet['lang']\n",
    "    author_id = tweet['author_id']\n",
    "\n",
    "    if 'context_annotations' in tweet:\n",
    "        entity = tweet['context_annotations'][0]['entity']\n",
    "        entity_id = entity['id']\n",
    "        entity_name = entity['name']\n",
    "        if 'description' in tweet['context_annotations'][0]['domain']:\n",
    "            author_des = tweet['context_annotations'][0]['domain']['description']\n",
    "            author_removal_list = []\n",
    "            author_des_tokens = set(tt.tokenize(author_des.lower()))\n",
    "\n",
    "            for token in author_des_tokens:\n",
    "                # if token is a link\n",
    "                if (token.startswith('https') or token.startswith('http')):\n",
    "                    author_removal_list.append(token)\n",
    "\n",
    "                # if token doesn't contain any letters\n",
    "                elif not any(char.isalpha() for char in token):\n",
    "                    author_removal_list.append(token)\n",
    "\n",
    "                # if token is found in stopwords\n",
    "                elif (token in stopwords):\n",
    "                    author_removal_list.append(token)\n",
    "\n",
    "            # remove all compiled tokens in for loop above\n",
    "            for token in author_removal_list:\n",
    "                author_des_tokens.remove(token)\n",
    "\n",
    "        else:\n",
    "            author_des_tokens = None\n",
    "    else:\n",
    "        entity_id = data['entity_name'] = data['author_des'] = None\n",
    "\n",
    "    if 'entities' in tweet:\n",
    "        hashtags = []\n",
    "        mentions = []\n",
    "        annotations = []\n",
    "        if 'hashtags' in tweet['entities']:\n",
    "            for hashtag in tweet['entities']['hashtags']:\n",
    "                hashtags.append(hashtag['tag'])\n",
    "        if 'mentions' in tweet['entities']:\n",
    "            for mention in tweet['entities']['mentions']:\n",
    "                mentions.append(mention['id'])\n",
    "        if 'annotations' in tweet['entities']:\n",
    "            for annotation in tweet['entities']['annotations']:\n",
    "                annotations.append((annotation['normalized_text'], annotation['type'], annotation['probability']))\n",
    "\n",
    "    # compiling data\n",
    "    data = [new_text, lang, author_id, entity_id, entity_name, author_des_tokens, hashtags, mentions, annotations]'''\n",
    "    data = text\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "70d0238d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1806 written to file, ( 89 invalid events )\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "train_invalid_events = []\n",
    "train_events_data = []    \n",
    "    \n",
    "for event in train_tweet_events:\n",
    "    is_rumour = event['is_rumour']\n",
    "    tweet_objects = []\n",
    "    event_list = \"[CLS] \"\n",
    "    valid = True\n",
    "   \n",
    "    for tweet_id in event['ids']:\n",
    "        try:\n",
    "            with open('./train_dev_data/train/' + tweet_id + '.json') as tweet_str:\n",
    "                for line in tweet_str:\n",
    "                    tweet = json.loads(line)\n",
    "                    tweet_objects.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if (len(tweet_objects) == 0):\n",
    "        train_invalid_events.append(index)\n",
    "        valid = False\n",
    "        \n",
    "    else:\n",
    "        for tweet in tweet_objects:            \n",
    "            get_features(tweet, features)\n",
    "            \n",
    "            data = feature_extraction(tweet)\n",
    "            event_list += data + \" [SEP]\"\n",
    "            \n",
    "    index += 1\n",
    "    if valid:\n",
    "        train_events_data.append((event_list, is_rumour))\n",
    "        \n",
    "print(len(train_events_data), \"written to training input file, (\", len(train_invalid_events), \"invalid events )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "34f6ce26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "594 written to development input file, ( 38 invalid events )\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "dev_invalid_events = []\n",
    "dev_events_data = []    \n",
    "    \n",
    "for event in dev_tweet_events:\n",
    "    is_rumour = event['is_rumour']\n",
    "    tweet_objects = []\n",
    "    event_list = \"[CLS] \"\n",
    "    valid = True\n",
    "   \n",
    "    for tweet_id in event['ids']:\n",
    "        try:\n",
    "            with open('./train_dev_data/dev/' + tweet_id + '.json') as tweet_str:\n",
    "                for line in tweet_str:\n",
    "                    tweet = json.loads(line)\n",
    "                    tweet_objects.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if (len(tweet_objects) == 0):\n",
    "        dev_invalid_events.append(index)\n",
    "        valid = False\n",
    "        \n",
    "    else:\n",
    "        for tweet in tweet_objects:            \n",
    "            data = feature_extraction(tweet)\n",
    "            event_list += data + \" [SEP]\"\n",
    "            \n",
    "    index += 1\n",
    "    if valid:\n",
    "        dev_events_data.append((event_list, is_rumour))\n",
    "\n",
    "print(len(dev_events_data), \"written to development input file, (\", len(dev_invalid_events), \"invalid events )\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "9bb566cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "header = [\"Event Tweets\", \"Label\"]\n",
    "\n",
    "train_csv = pd.DataFrame(columns = header, data = train_events_data)\n",
    "train_csv.to_csv('./train_dev_data/train.csv')\n",
    "dev_csv = pd.DataFrame(columns = header, data = dev_events_data)\n",
    "dev_csv.to_csv('./train_dev_data/dev.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8867066",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "67922b3c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context_annotations': {'domain': {'id': 1561,\n",
       "   'name': 1561,\n",
       "   'description': 1561},\n",
       "  'entity': {'id': 1561, 'name': 1561, 'description': 437}},\n",
       " 'reply_settings': 7576,\n",
       " 'public_metrics': {'retweet_count': 7576,\n",
       "  'reply_count': 7576,\n",
       "  'like_count': 7576,\n",
       "  'quote_count': 7576},\n",
       " 'attachments': {'media_keys': 1474, 'poll_ids': 10},\n",
       " 'entities': {'urls': 2315,\n",
       "  'hashtags': 1250,\n",
       "  'mentions': 6201,\n",
       "  'annotations': 2256},\n",
       " 'created_at': 7576,\n",
       " 'author_id': 7576,\n",
       " 'lang': 7576,\n",
       " 'conversation_id': 7576,\n",
       " 'text': 7576,\n",
       " 'id': 7576,\n",
       " 'possibly_sensitive': 7576,\n",
       " 'source': 7569,\n",
       " 'in_reply_to_user_id': 7141,\n",
       " 'referenced_tweets': 7150,\n",
       " 'geo': {'place_id': 263, 'coordinates': 144},\n",
       " 'withheld': {'copyright': 2, 'country_codes': 2}}"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# obtaining features and their counts from the tweet object obtained from the training set\n",
    "# (total of 21775 tweet objects)\n",
    "def get_features(tweet, features):\n",
    "    for feat in tweet:\n",
    "        if feat == 'context_annotations':\n",
    "            for feat2 in tweet[feat][0]:\n",
    "                if type(tweet[feat][0][feat2]) == dict:\n",
    "                    for feat3 in tweet[feat][0][feat2]:\n",
    "                        if feat in features:\n",
    "                            if feat2 in features[feat]:\n",
    "                                if feat3 in features[feat][feat2]:\n",
    "                                    features[feat][feat2][feat3] += 1\n",
    "                                else:\n",
    "                                    features[feat][feat2][feat3] = 1\n",
    "                            else:\n",
    "                                features[feat][feat2] = dict()\n",
    "                                features[feat][feat2][feat3] = 1\n",
    "                        else:\n",
    "                            features[feat] = dict()\n",
    "                            features[feat][feat2] = dict()\n",
    "                            features[feat][feat2][feat3] = 1\n",
    "                else:\n",
    "                    if feat2 in features[feat]:\n",
    "                        features[feat][feat2] += 1\n",
    "                    else:\n",
    "                        features[feat][feat2] = 1\n",
    "        elif type(tweet[feat]) == dict:\n",
    "            for feat2 in tweet[feat]:\n",
    "                if feat in features:\n",
    "                    if feat2 in features[feat]:\n",
    "                        features[feat][feat2] += 1\n",
    "                    else:\n",
    "                        features[feat][feat2] = 1\n",
    "                else:\n",
    "                    features[feat] = dict()\n",
    "                    features[feat][feat2] = 1\n",
    "        else:\n",
    "            if feat in features:\n",
    "                features[feat] += 1\n",
    "            else:\n",
    "                features[feat] = 1\n",
    "                \n",
    "features = dict()\n",
    "features['context_annotations'] = dict()\n",
    "                \n",
    "for event in dev_tweet_events:\n",
    "    tweet_objects = []\n",
    "   \n",
    "    for tweet_id in event['ids']:\n",
    "        try:\n",
    "            with open('./train_dev_data/dev/' + tweet_id + '.json') as tweet_str:\n",
    "                for line in tweet_str:\n",
    "                    tweet = json.loads(line)\n",
    "                    tweet_objects.append(tweet)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    if (len(tweet_objects) != 0):\n",
    "        for tweet in tweet_objects:            \n",
    "            get_features(tweet, features)\n",
    "\n",
    "features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
